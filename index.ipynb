{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Language Translation with seq2seq Models - Codealong\n",
    "\n",
    "## Introduction\n",
    "\n",
    "So far, we have looked at building simple  and deep AEs for data compression and de-noising. We also looked at how CNN based AE architectures can produce improved results for compression and de-noising tasks in image analysis.\n",
    "\n",
    "In this lab, we will look at how to implement a basic character-level sequence-to-sequence model using RNNs, following a similar encoder-decoder approach. The model will be able to translate short English phrases into short French, in a character-by-character fashion. Such a model can be further modified to process whole words instead of characters. You are encouraged to consider this experiment as a baseline model which you can further scale and modify according to an analytical question or a specific business need. \n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- Understand and describe a sequence to sequence architecture for language modeling \n",
    "- Pre-process a given text corpus to be trained under a teacher forcing approach\n",
    "- Build a simple yet scalable machine translation deep seq2seq model using LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence (seq2seq) Models \n",
    "\n",
    "The sequence to sequence (seq2seq) model is a deep learning model that converts an input sequence into an output sequence. In this context, the sequence could be a list of symbols, corresponding to the words in a sentence. The seq2seq model has achieved great success in fields such as machine translation, chat bots and text summarization. All of these tasks can be regarded as the task to learn a model that converts an input sequence into an output sequence.\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "The architecture of seq2seq model can be separated to the five major roles.\n",
    "\n",
    "- Encoder Input Layer (Embedding)\n",
    "- Encoder Recurrent Layer\n",
    "- Decoder Input Layer (Embedding)\n",
    "- Decoder Recurrent Layer\n",
    "- Decoder Output Layer\n",
    "\n",
    "Below is a highly simplified representation of this approach where we try to compress the input data and reproduce it as a standard function of an AE. \n",
    "\n",
    "<img src=\"s2s1.png\">\n",
    "\n",
    "[Follow this link](http://docs.chainer.org/en/stable/examples/seq2seq.html) to see a detailed version of this model describing the role of every component and maths behind the processing that takes place at each stage. Here we will briefly look at the role of each of above components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Model and the \"Thought Vector\"\n",
    "Above we see that the encoder model is supplied with input \"HEY\".  For a character level model, we first split the complete sentence down to its character level. So first \"H\" is fed into the network and an intermediate state vector (known as a **Thought Vector** , shown in yellow) is formed containing the learned information of \"H\". **The thought vector is used for remembering key features of our input data as a standard function of an encoder**. Followed by this, the thought vector is fed into the second RNN input and it combines the information seen thus far (i.e. \"H\") with the new data (\"E\"). Finally, \"Y\" is added to the thought vector. The procedure so far is done by a so-called encoder. It takes characters (or other items) as input and it forms a thought vector as output.\n",
    "\n",
    "> **“Thought vector”** is a term popularized by Geoffrey Hinton, the prominent deep-learning researcher now at Google. It is a very interesting topic and you can optionally [follow this link](https://skymind.ai/wiki/thought-vectors) to read an article for a detailed understanding with wider applications of thought vectors.\n",
    "\n",
    "### Decoder Model \n",
    "\n",
    "In a simple seq2seq approach, the decoder model inputs the thought vector and produces an output. Based on the business need, We can train our seq2seq model for a number of possible applications. We can buils a chatbot using such an architecture model but by using words instead of characters. We can also train such a network for language translation. Other variants of such modeling can also offer features like image captioning, generating new images from text etc. \n",
    "\n",
    "The key strength  for this architecture is that the decoder is able to convert some numbers back to a text. This allows to generate some text in response to some given text. If the thought vector is small enough, then all input data is compressed into a small vector which contains just enough information. As an AE, The model also acts as a compressor as it learns how to store information as efficient as possible into the thought vector and how to decode the compression into text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Translation \n",
    "\n",
    "Next, we will look at using a seq2seq model , similar to the one shown above to create a machine translation architecture. \n",
    "\n",
    "> **Machine translation, sometimes referred to by the abbreviation MT is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another.**\n",
    "\n",
    "A trained model would take an input string in english language and attempt to translate it into French, similar to example shown below\n",
    "\n",
    "### \"the cat sat on the mat\" -> `[Seq2Seq model] ` -> \"le chat etait assis sur le tapis\"\n",
    "\n",
    "View [this paper](https://arxiv.org/pdf/1703.01619.pdf) for a deep dive into neural machine translation with seq2seq approach. \n",
    "\n",
    "## Dataset\n",
    "\n",
    "For this lab, we will use a dataset of pairs of English sentences and their French translation, which you can download from [Anki](http://manythings.org/anki). This site contains translations in a number of other languages and you are welcome to down a different dataset. The file we are using is `fra-eng.zip`. We will implement a character-level sequence-to-sequence model, processing the input character-by-character and generating the output character-by-character. \n",
    "\n",
    "We will perform following tasks to get our text data ready for processing in the first stage. \n",
    "\n",
    "- Read the dataset line by line and split to separate input (English) from target (French translation)\n",
    "- For first 10000 entries: \n",
    "    - Create Input and Target dataset containing corresponding text \n",
    "    - Create character level vocabularies for both input and target datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Read the input file - line by line \n",
    "with open('fra.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "# Target characters set\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "# Define number of samples to train on (you can increase or decrease this number as desired)\n",
    "num_samples = 10000  \n",
    "\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    \n",
    "    # Read input and target - separated by \"tab\" \n",
    "    input_text, target_text = line.split('\\t')\n",
    "    \n",
    "    # \"tab\" as the \"start sequence\" character for the targets, \n",
    "    # \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "   \n",
    "    # Append input and target text to vectors\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    # Create input and target, characater level sets\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now sort the input and target character sets and check for length of our character level vocabularies comprised of characters as tokens. We will also check for the maximum length of sequence for input and target datasets to later define the architecture of our seq2seq model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 69\n",
      "Number of unique output tokens: 93\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "# Sort input and target character level vocabularies\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "\n",
    "# Calculate the length of vocabularies\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "\n",
    "# Identify sequences of maximum length for both encoder and decoder models\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "# Print the calculated information in a nice manner\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly see what our vocabularies look like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '$', '%', '&', \"'\", ',', '-', '.', '0', '1', '3', '5', '6', '7', '8', '9', ':', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      "['\\t', '\\n', ' ', '!', '$', '%', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '3', '5', '8', '9', ':', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', '«', '»', 'À', 'Ç', 'É', 'Ê', 'à', 'â', 'ç', 'è', 'é', 'ê', 'ë', 'î', 'ï', 'ô', 'ù', 'û', 'œ', '\\u2009', '’', '\\u202f']\n"
     ]
    }
   ],
   "source": [
    "# Print sorted character lists\n",
    "print (input_characters)\n",
    "print()\n",
    "print (target_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have successfully created the vocabularies including uppercase/lowercase characters and punctuation marks present in both datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Embeddings and Target \n",
    "\n",
    "We will now create 3 Numpy arrays:\n",
    "- `encoder_input_data` \n",
    "- `decoder_input_data` \n",
    "- `decoder_target_data` \n",
    "\n",
    "using the input and target texts datasets created above. \n",
    "\n",
    "- `encoder_input_data` is a 3D array of shape `(num_pairs, max_english_sentence_length, num_english_characters)` containing a one-hot vectorization of the English sentences.\n",
    "\n",
    "- `decoder_input_data` is a 3D array of shape `(num_pairs, max_french_sentence_length, num_french_characters)` containg a one-hot vectorization of the French sentences.\n",
    "\n",
    "- `decoder_target_data` is the same as `decoder_input_data` but offset by one timestep. `decoder_target_data[:, t, :]` will be the same as `decoder_input_data[:, t + 1, :]` using the teacher forcing model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create indices for input and target character level datasets\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros((len(input_texts), \n",
    "                               max_encoder_seq_length, \n",
    "                               num_encoder_tokens), \n",
    "                              dtype='float32')\n",
    "\n",
    "decoder_input_data = np.zeros((len(input_texts), \n",
    "                               max_decoder_seq_length, \n",
    "                               num_decoder_tokens),\n",
    "                              dtype='float32')\n",
    "\n",
    "decoder_target_data = np.zeros((len(input_texts), \n",
    "                                max_decoder_seq_length, \n",
    "                                num_decoder_tokens),\n",
    "                               dtype='float32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, the lengths of our vocabularies and total samples are now exactly was needed. This takes cares of our preprocessing stage. Let's move on to the modeling stage next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Encoder and Decoder Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Encoder/Decoder with Teacher Forcing\n",
    "- Start with input sequences (word or character level) from a domain (English) and corresponding target sequences from another domain (French).\n",
    "\n",
    "- Use LSTM for encoder, turning input sequences to 2 state vectors (i.e. keep the last LSTM state, discarding the outputs).\n",
    "\n",
    "- Following a \"Teacher Forcing\" approach (visit link below for details), use LSTM for decoder and trained it to turn the target sequences into the same sequence but offset by one timestep (i.e. on character) in the future. \n",
    "\n",
    "> Effectively, the decoder learns to generate `targets[t+1...]` given `targets[...t]`, conditioned on the input sequence.Here is a excellent resource explaining [Techer Forcing]([Teacher Forcing](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/) in detail. \n",
    "\n",
    "<img src=\"s2s3.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data offset for Teacher forcing model \n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    \n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    \n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        \n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the seq2seq Model\n",
    "\n",
    "With our pre-processed dataset, we will perform following tasks to achieve the functionality highlighted above. \n",
    "\n",
    "- Train a basic LSTM-based Seq2Seq model to predict `decoder_target_data` given `encoder_input_data` and `decoder_input_data` using teacher forcing.\n",
    "- Define an input sequence in keras using `num_encoder_tokens` for input shape\n",
    "- Discard encoder outputs and only keep the LSTM state.\n",
    "- Set up the decoder, using encoder state from above as initial state.\n",
    "- Set up the decoder to return full output sequences,and to return internal states as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries - keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/steeznation/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Encoder LSTM\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# Discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder LSTM\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Seq2Seq model\n",
    "Now we can put together our encoder-decoder architecture by defining a model that will change `encoder_input_data` & `decoder_input_data` into `decoder_target_data`. As we a finite dictionary, we will use `categorical_crossentropy` as our loss measure. We will use `rmsprop` optimization that is a standard measure for such categorical data. We will train the model for 50 epochs (this will take a while to train) with a train/test split of 80/20. \n",
    "\n",
    "*We can optionally save the trained model as shown below. If you are not interested in running the complete training process, we have already trained this network and saved it for you. You can simply reload the saved model `eng-french.h5` available in the repo and carry on with rest of this lab. [Here is a quick reference for saving and retrieving models in Keras](https://jovianlin.io/saving-loading-keras-models/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/steeznation/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/steeznation/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.9217 - val_loss: 0.9396\n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.7266 - val_loss: 0.7779\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.6180 - val_loss: 0.7095\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 0.5635 - val_loss: 0.6348\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.5224 - val_loss: 0.6152\n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.4907 - val_loss: 0.5762\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.4634 - val_loss: 0.5583\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.4405 - val_loss: 0.5286\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 0.4206 - val_loss: 0.5199\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 0.4025 - val_loss: 0.5042\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.3864 - val_loss: 0.4952\n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 0.3720 - val_loss: 0.4837\n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.3581 - val_loss: 0.4821\n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.3445 - val_loss: 0.4662\n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.3328 - val_loss: 0.4617\n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.3209 - val_loss: 0.4690\n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.3100 - val_loss: 0.4574\n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.2992 - val_loss: 0.4594\n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 0.2896 - val_loss: 0.4587\n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.2800 - val_loss: 0.4568\n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.2711 - val_loss: 0.4553\n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.2624 - val_loss: 0.4555\n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.2543 - val_loss: 0.4578\n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.2466 - val_loss: 0.4653\n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.2390 - val_loss: 0.4552\n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.2320 - val_loss: 0.4661\n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.2252 - val_loss: 0.4654\n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.2186 - val_loss: 0.4685\n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.2121 - val_loss: 0.4757\n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.2064 - val_loss: 0.4752\n",
      "Epoch 31/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.2007 - val_loss: 0.4839\n",
      "Epoch 32/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.1950 - val_loss: 0.4782\n",
      "Epoch 33/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.1896 - val_loss: 0.4870\n",
      "Epoch 34/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.1849 - val_loss: 0.4937\n",
      "Epoch 35/50\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 0.1802 - val_loss: 0.4933\n",
      "Epoch 36/50\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.1754 - val_loss: 0.4956\n",
      "Epoch 37/50\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.1709 - val_loss: 0.5025\n",
      "Epoch 38/50\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.1665 - val_loss: 0.5102\n",
      "Epoch 39/50\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 0.1625 - val_loss: 0.5124\n",
      "Epoch 40/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.1588 - val_loss: 0.5135\n",
      "Epoch 41/50\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 0.1548 - val_loss: 0.5172\n",
      "Epoch 42/50\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.1513 - val_loss: 0.5213\n",
      "Epoch 43/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.1477 - val_loss: 0.5350\n",
      "Epoch 44/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.1443 - val_loss: 0.5328\n",
      "Epoch 45/50\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 0.1409 - val_loss: 0.5375\n",
      "Epoch 46/50\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.1377 - val_loss: 0.5444\n",
      "Epoch 47/50\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.1349 - val_loss: 0.5495\n",
      "Epoch 48/50\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.1320 - val_loss: 0.5557\n",
      "Epoch 49/50\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.1287 - val_loss: 0.5582\n",
      "Epoch 50/50\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.1263 - val_loss: 0.5658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steeznation/anaconda3/lib/python3.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# Train the Model \n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 50  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "\n",
    "# Define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], \n",
    "          decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "# Save model\n",
    "model.save('eng-french.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we now have a our trained model which we can use to make inferences for a given input sequence. Let's see how to achieve this. DO notice that our network is overfitting. Ideally, this should be dealt with using cross validation and much larger datasets. We will ignore this for now and move on towards generating some predictions from this trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences from Trained Model  \n",
    "\n",
    "In order to make inference from the trained model above, we will encode the input sentence and retrieve the initial decoder state. We will run one step of the decoder with this initial state and a \"start of sequence\" token as target. The output will be the next target character. We will append the target character prediction and repeat. The process of extracting some piece of learned information from a neural network is referred to as inferencing. [Read this interesting article](https://blogs.nvidia.com/blog/2016/08/22/difference-deep-learning-training-inference-ai/) to see how to differentiate between training and inferencing. \n",
    "\n",
    "<img src=\"s2s2.png\">\n",
    "\n",
    "Let's first build an inferencing setup using the model trained above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference setup for sampling.\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder Input\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# Decoder output\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# build a decoder model\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decode unknown input sequences for this experiment, we will:\n",
    "\n",
    "- Encode the input sequence into state vectors\n",
    "- Start with a target sequence of size 1\n",
    "- Feed the state vectors and 1-char target sequence to the decoder to produce predictions for the next character\n",
    "- Sample the next character using these predictions by using argmax\n",
    "- Append the sampled character to the target sequence\n",
    "- Repeat until the end-of-sequence character or character limit.\n",
    "\n",
    "We will now use above to implement the inference loop described above, as a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_input_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pass in some english text as input embeddings to this function and it should produce a french translation. Let's just borrow some text from our training set and pass it in. Let's pass in entries from line 0 to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Va !\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Attrapez !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Courez !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Courez !\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Attrapez !\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Fais-lui un câlin !\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Tout le monde peutte.\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Va à t'intérieure.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Cours-ez le pare !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Cours-ez le pare !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Cours-ez le pare !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Vas-y !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Vas-y !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Vas-y !\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je vous ai sauvées.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Je vous ai sauvée.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(0,20):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_input_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks great, partly because our model showed some over-fitting and partly - because it actually works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level up - Optional \n",
    "\n",
    "- Split the dataset into train and test sets and try to make predictions for previously unseen data.\n",
    "- Create new phrases and pass them in as input embeddings\n",
    "- USe GRUs instead of LSTMs and monitor the effect on predictive performance\n",
    "- Build a word level translation model using word2vec approach seen earlier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "- [Neural Machine Translation](https://medium.com/@dev.elect.iitd/neural-machine-translation-using-word-level-seq2seq-model-47538cba8cd7)\n",
    "- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
    "- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "In this code along styled lab we looked at building a machine learning architecture using a seq2seq model with LSTMs. The purpose of this lab is to provide you with a basic understanding of neural machine translation and to provide you with a code base that you can scale and modify. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
